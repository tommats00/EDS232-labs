{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Methods\n",
    "This notebook is a competition for my machine learning (EDS-232) class taught by Matteo Robbins. In this exercise, I will demonstrate machine learning methods to predict dissolved inorganic carbon (DIC) levels in water samples collected by the Califronia Cooperative Oceanic Fisheries Investigations program. This data was downloaded from the CalCOFI data portal, where bottle and cast data was merged, and releveant variables were selected. Data is split into a training and testing set. Machine learning models will be trained on the training set and then evaluated on the testing set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using multiple methods\n",
    "\n",
    "The goal of this exercise will be to demonstrate and optimize multiple machine learning methods. I am particularly interested in running a decision tree, random forest, and deep learning model for my data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train = pd.read_csv('data/train.csv')\n",
    "\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Lat_Dec</th>\n",
       "      <th>Lon_Dec</th>\n",
       "      <th>NO2uM</th>\n",
       "      <th>NO3uM</th>\n",
       "      <th>NH3uM</th>\n",
       "      <th>R_TEMP</th>\n",
       "      <th>R_Depth</th>\n",
       "      <th>R_Sal</th>\n",
       "      <th>R_DYNHT</th>\n",
       "      <th>R_Nuts</th>\n",
       "      <th>R_Oxy_micromol.Kg</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>PO4uM</th>\n",
       "      <th>SiO3uM</th>\n",
       "      <th>TA1.x</th>\n",
       "      <th>Salinity1</th>\n",
       "      <th>Temperature_degC</th>\n",
       "      <th>DIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>34.385030</td>\n",
       "      <td>-120.665530</td>\n",
       "      <td>0.03</td>\n",
       "      <td>33.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.79</td>\n",
       "      <td>323</td>\n",
       "      <td>141.2</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.40948</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.77</td>\n",
       "      <td>53.86</td>\n",
       "      <td>2287.45</td>\n",
       "      <td>34.198</td>\n",
       "      <td>7.82</td>\n",
       "      <td>2270.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>31.418333</td>\n",
       "      <td>-121.998333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>34.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.12</td>\n",
       "      <td>323</td>\n",
       "      <td>140.8</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.81441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.57</td>\n",
       "      <td>52.50</td>\n",
       "      <td>2279.10</td>\n",
       "      <td>34.074</td>\n",
       "      <td>7.15</td>\n",
       "      <td>2254.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>34.385030</td>\n",
       "      <td>-120.665530</td>\n",
       "      <td>0.18</td>\n",
       "      <td>14.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.68</td>\n",
       "      <td>50</td>\n",
       "      <td>246.8</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.29150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.29</td>\n",
       "      <td>13.01</td>\n",
       "      <td>2230.80</td>\n",
       "      <td>33.537</td>\n",
       "      <td>11.68</td>\n",
       "      <td>2111.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    Lat_Dec     Lon_Dec  NO2uM  NO3uM  NH3uM  R_TEMP  R_Depth  R_Sal  \\\n",
       "0   1  34.385030 -120.665530   0.03   33.8    0.0    7.79      323  141.2   \n",
       "1   2  31.418333 -121.998333   0.00   34.7    0.0    7.12      323  140.8   \n",
       "2   3  34.385030 -120.665530   0.18   14.2    0.0   11.68       50  246.8   \n",
       "\n",
       "   R_DYNHT  R_Nuts  R_Oxy_micromol.Kg  Unnamed: 12  PO4uM  SiO3uM    TA1.x  \\\n",
       "0    0.642     0.0           37.40948          NaN   2.77   53.86  2287.45   \n",
       "1    0.767     0.0           64.81441          NaN   2.57   52.50  2279.10   \n",
       "2    0.144     0.0          180.29150          NaN   1.29   13.01  2230.80   \n",
       "\n",
       "   Salinity1  Temperature_degC      DIC  \n",
       "0     34.198              7.82  2270.17  \n",
       "1     34.074              7.15  2254.10  \n",
       "2     33.537             11.68  2111.04  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View head of the dataset\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                      0\n",
       "Lat_Dec                 0\n",
       "Lon_Dec                 0\n",
       "NO2uM                   0\n",
       "NO3uM                   0\n",
       "NH3uM                   0\n",
       "R_TEMP                  0\n",
       "R_Depth                 0\n",
       "R_Sal                   0\n",
       "R_DYNHT                 0\n",
       "R_Nuts                  0\n",
       "R_Oxy_micromol.Kg       0\n",
       "Unnamed: 12          1454\n",
       "PO4uM                   0\n",
       "SiO3uM                  0\n",
       "TA1.x                   0\n",
       "Salinity1               0\n",
       "Temperature_degC        0\n",
       "DIC                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1454, 19)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of the dataset\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 1454 rows in our training data. From our observations we also see that the column `Unnamed: 12` has missing values for every row. This data is useless so we can initially drop this column entirely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   0\n",
       "Lat_Dec              0\n",
       "Lon_Dec              0\n",
       "NO2uM                0\n",
       "NO3uM                0\n",
       "NH3uM                0\n",
       "R_TEMP               0\n",
       "R_Depth              0\n",
       "R_Sal                0\n",
       "R_DYNHT              0\n",
       "R_Nuts               0\n",
       "R_Oxy_micromol.Kg    0\n",
       "PO4uM                0\n",
       "SiO3uM               0\n",
       "TA1.x                0\n",
       "Salinity1            0\n",
       "Temperature_degC     0\n",
       "DIC                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the column\n",
    "train = train.drop('Unnamed: 12', axis = 1)\n",
    "\n",
    "# Check if it dropped\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                     int64\n",
       "Lat_Dec              float64\n",
       "Lon_Dec              float64\n",
       "NO2uM                float64\n",
       "NO3uM                float64\n",
       "NH3uM                float64\n",
       "R_TEMP               float64\n",
       "R_Depth                int64\n",
       "R_Sal                float64\n",
       "R_DYNHT              float64\n",
       "R_Nuts               float64\n",
       "R_Oxy_micromol.Kg    float64\n",
       "PO4uM                float64\n",
       "SiO3uM               float64\n",
       "TA1.x                float64\n",
       "Salinity1            float64\n",
       "Temperature_degC     float64\n",
       "DIC                  float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data types of the columns\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our variables are composed of `float64`s and `int64`s meaning that they are numerical values. So, we can't use any classification models. The models I am initially interested in are a decision tree, random forest, and a deep learning model. Let's run through each one, one at a time, starting with a decision tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = train.drop(columns =['DIC'])\n",
    "y = train['DIC']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 808)\n",
    "\n",
    "# Scale data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Decision Tree and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor()\n",
    "\n",
    "# Define the parameters\n",
    "param_dist = {\n",
    "    'max_depth': [1, 2, 3, 4, 5, None],\n",
    "    'min_samples_split': [1, 2, 3, 4, 5, 6, 7],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Tune the hyperparameters w/ GridSearchCV\n",
    "random_search = RandomizedSearchCV(dt, \n",
    "                           param_dist, \n",
    "                           cv = 5, ## 5-fold cross validation\n",
    "                           n_jobs = -1) ## Use all cores \n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_dt_params = random_search.best_params_\n",
    "print(f\"Best Hyperparameters: {best_dt_params}\")\n",
    "\n",
    "# Define best model variable for evaluation\n",
    "best_dt = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on our Training data 'test set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree RMSE: 9.106\n"
     ]
    }
   ],
   "source": [
    "# Predict on the best model\n",
    "y_dt_pred = best_dt.predict(X_test)\n",
    "\n",
    "# Calculate RMSE from the best model\n",
    "dt_rmse = np.sqrt(mean_squared_error(y_test, y_dt_pred))\n",
    "print(f\"Decision Tree RMSE: {dt_rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have calcualated the RMSE from our training data set in our model. Now, let's use the test set we intially loaded to evaluate how well our model will be on \"New\", \"Unknown\" data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare testing and training data \n",
    "# Make sure columns are the same as the training data\n",
    "\n",
    "# Create a conditional statement to check if the columns are the same\n",
    "\n",
    "test.columns.equals(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in test dataset but not in train dataset: {'TA1'}\n",
      "Columns in train dataset but not in test dataset: {'DIC', 'TA1.x'}\n"
     ]
    }
   ],
   "source": [
    "# Get the column names of both datasets\n",
    "train_columns = set(train.columns)\n",
    "test_columns = set(test.columns)\n",
    "\n",
    "# Find the differences in column names\n",
    "missing_in_train = test_columns - train_columns\n",
    "missing_in_test = train_columns - test_columns\n",
    "\n",
    "# Show the differences\n",
    "if missing_in_train or missing_in_test:\n",
    "    if missing_in_train:\n",
    "        print(f\"Columns in test dataset but not in train dataset: {missing_in_train}\")\n",
    "    if missing_in_test:\n",
    "        print(f\"Columns in train dataset but not in test dataset: {missing_in_test}\")\n",
    "else:\n",
    "    print(\"The datasets have the same column names.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see which columns are missing in each dataset. From the looks of it, it seems that the variable `TA1` and `TA1.x` are supposed to be the same, but are just spelled differently. We can fix that by renaming one to the other. It also look `DIC` is not in our testing set. This is expected as this is our target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Lat_Dec</th>\n",
       "      <th>Lon_Dec</th>\n",
       "      <th>NO2uM</th>\n",
       "      <th>NO3uM</th>\n",
       "      <th>NH3uM</th>\n",
       "      <th>R_TEMP</th>\n",
       "      <th>R_Depth</th>\n",
       "      <th>R_Sal</th>\n",
       "      <th>R_DYNHT</th>\n",
       "      <th>R_Nuts</th>\n",
       "      <th>R_Oxy_micromol.Kg</th>\n",
       "      <th>PO4uM</th>\n",
       "      <th>SiO3uM</th>\n",
       "      <th>TA1.x</th>\n",
       "      <th>Salinity1</th>\n",
       "      <th>Temperature_degC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1455</td>\n",
       "      <td>34.321666</td>\n",
       "      <td>-120.811666</td>\n",
       "      <td>0.02</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>9.51</td>\n",
       "      <td>101</td>\n",
       "      <td>189.9</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.41</td>\n",
       "      <td>138.838300</td>\n",
       "      <td>1.85</td>\n",
       "      <td>25.5</td>\n",
       "      <td>2244.94</td>\n",
       "      <td>33.830</td>\n",
       "      <td>9.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1456</td>\n",
       "      <td>34.275000</td>\n",
       "      <td>-120.033333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.84</td>\n",
       "      <td>102</td>\n",
       "      <td>185.2</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.709200</td>\n",
       "      <td>2.06</td>\n",
       "      <td>28.3</td>\n",
       "      <td>2253.27</td>\n",
       "      <td>33.963</td>\n",
       "      <td>9.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1457</td>\n",
       "      <td>34.275000</td>\n",
       "      <td>-120.033333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>31.9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.60</td>\n",
       "      <td>514</td>\n",
       "      <td>124.1</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.174548</td>\n",
       "      <td>3.40</td>\n",
       "      <td>88.1</td>\n",
       "      <td>2316.95</td>\n",
       "      <td>34.241</td>\n",
       "      <td>6.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id    Lat_Dec     Lon_Dec  NO2uM  NO3uM  NH3uM  R_TEMP  R_Depth  R_Sal  \\\n",
       "0  1455  34.321666 -120.811666   0.02   24.0   0.41    9.51      101  189.9   \n",
       "1  1456  34.275000 -120.033333   0.00   25.1   0.00    9.84      102  185.2   \n",
       "2  1457  34.275000 -120.033333   0.00   31.9   0.00    6.60      514  124.1   \n",
       "\n",
       "   R_DYNHT  R_Nuts  R_Oxy_micromol.Kg  PO4uM  SiO3uM    TA1.x  Salinity1  \\\n",
       "0    0.258    0.41         138.838300   1.85    25.5  2244.94     33.830   \n",
       "1    0.264    0.00         102.709200   2.06    28.3  2253.27     33.963   \n",
       "2    0.874    0.00           2.174548   3.40    88.1  2316.95     34.241   \n",
       "\n",
       "   Temperature_degC  \n",
       "0              9.52  \n",
       "1              9.85  \n",
       "2              6.65  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename TA1 to TA1.x to match our training data\n",
    "test = test.rename(columns = {'TA1': 'TA1.x'})\n",
    "\n",
    "# Check to see if the column was renamed\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make predictions on our test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tdude\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "y_pred_dt = best_dt.predict(test)\n",
    "\n",
    "# Create a dataframe containing ID and DIC for compeititon submission\n",
    "test['DIC'] = y_pred_dt\n",
    "submission = test[['id', 'DIC']]\n",
    "\n",
    "# Save the submission to a csv \n",
    "# submission.to_csv('submission_dt.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 7, 'max_features': 6, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Define hyperparameters\n",
    "param_grid = {\n",
    "    'max_features': ['sqrt', 6, None],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3,4,5,6,7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(\n",
    "    rf,\n",
    "    param_grid = param_grid,\n",
    "    cv = 5, \n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the best hyperparameters\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Define the best model\n",
    "best_rf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 7.047\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m feature_importance \u001b[38;5;241m=\u001b[39m best_rf\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Extract the feature importances\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m importance_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mzip\u001b[39m(\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m, best_rf),\n\u001b[0;32m     13\u001b[0m                                    columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImportance\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Sort the feature importances\u001b[39;00m\n\u001b[0;32m     16\u001b[0m importance_df \u001b[38;5;241m=\u001b[39m importance_df\u001b[38;5;241m.\u001b[39msort_values(by \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImportance\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# Predict on the best model\n",
    "y_rf_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, y_rf_pred))\n",
    "print(f\"Mean Squared Error: {rf_rmse:.3f}\")\n",
    "\n",
    "# Extract features importances\n",
    "feature_importance = best_rf.feature_importances_\n",
    "\n",
    "# Extract the feature importances\n",
    "importance_df = pd.DataFrame(zip(X_test.columns, best_rf),\n",
    "                                   columns = ['Feature', 'Importance'])\n",
    "\n",
    "# Sort the feature importances\n",
    "importance_df = importance_df.sort_values(by = 'Importance', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'importance_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Visualize feature importance\u001b[39;00m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m sns\u001b[38;5;241m.\u001b[39mbarplot(y \u001b[38;5;241m=\u001b[39m \u001b[43mimportance_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m], x \u001b[38;5;241m=\u001b[39m importance_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImportance\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#sns.barplot(x = feature_importance, y = X.columns)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature Importance for determining nClimates\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'importance_df' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.barplot(y = importance_df['Feature'], x = importance_df['Importance'])\n",
    "#sns.barplot(x = feature_importance, y = X.columns)\n",
    "plt.title('Feature Importance for determining nClimates')\n",
    "plt.ylabel('Feature')\n",
    "plt.xlabel('Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [   0    1    2 ...  998  999 1000], got [1971.14 1983.53 1985.08 ... 2351.49 2353.97 2367.8 ]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 10\u001b[0m\n\u001b[0;32m      2\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m XGBClassifier(\n\u001b[0;32m      3\u001b[0m     n_estimators \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m      4\u001b[0m     learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m      5\u001b[0m     eval_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m     early_stopping_rounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m      7\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m808\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Fit the model and specify validation sets\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Best number of trees from baseline model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m best_ntree \u001b[38;5;241m=\u001b[39m xgb_model\u001b[38;5;241m.\u001b[39mbest_iteration\n",
      "File \u001b[1;32mc:\\Users\\tdude\\anaconda3\\envs\\ml-env\\lib\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tdude\\anaconda3\\envs\\ml-env\\lib\\site-packages\\xgboost\\sklearn.py:1471\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1468\u001b[0m     classes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1469\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m   1470\u001b[0m ):\n\u001b[1;32m-> 1471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1472\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1473\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1474\u001b[0m     )\n\u001b[0;32m   1476\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [   0    1    2 ...  998  999 1000], got [1971.14 1983.53 1985.08 ... 2351.49 2353.97 2367.8 ]"
     ]
    }
   ],
   "source": [
    "# Train a XGBoost model\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators = 1000,\n",
    "    learning_rate = 0.1,\n",
    "    eval_metric = 'logloss',\n",
    "    early_stopping_rounds = 50,\n",
    "    random_state = 808)\n",
    "\n",
    "# Fit the model and specify validation sets\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose = 0)\n",
    "\n",
    "# Best number of trees from baseline model\n",
    "best_ntree = xgb_model.best_iteration\n",
    "print(f\"The best number of trees is: {best_ntree}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
