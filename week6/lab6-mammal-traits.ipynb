{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Random Forest Regression on Malagasy Mammal Traits\n",
    "\n",
    "### Introduction\n",
    " In Lab 5, we used functional trait data on bird species to predict their presence in a given climate type. This week we are using regression tree models on a similar dataset, this one on mammal species.  Our goal is to predict the total number of climate types each mammal species inhabits.\n",
    "\n",
    "You will compare performance of a single decision tree with a tuned random forest to get a sense of the effectiveness of ensemble methods with optimized hyperparameter values.\n",
    "\n",
    "There is a fair amount of missing data in this set, which can lead to low reliability of feature splits, increased overfitting, and decreased accuracy of tree models. You will address this issue with a combination of variable omission and imputation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read data and retain a subset of the columns\n",
    "Similar to last week, we will need to take some steps to get the data in a usable form. We again reformat the diet-related variable names to match this set: `Diet_Vertebrates`,\n",
    "    `Diet_Fruits`, `Diet_Flowers`, `Diet_Seeds`, `Diet_Plants`, `Diet_Other`. Then drop columns that are not relevant functional traits or climate type variables.   Next, drop any remaining  variables that have greater than 40% of the observations missing. Print the final dataframe shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dat = pd.read_csv(\"/courses/EDS232/Data/MamTraitData.csv\", encoding=\"ISO-8859-1\").rename(columns={\n",
    "    'Diet: invertebrates': 'Diet_Invertebrates',\n",
    "    'Diet: vertebrates': 'Diet_Vertebrates',\n",
    "    'Diet: fruits': 'Diet_Fruits',\n",
    "    'Diet: flower/nectar/pollen/gums': 'Diet_Flowers',\n",
    "    'Diet: seeds': 'Diet_Seeds',\n",
    "    'Diet: other plant materials': 'Diet_Plants',\n",
    "    'Diet: scavenge; garbage; carrion; carcasses': 'Diet_Other'\n",
    "})\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Final DataFrame shape: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Imputation with KNN\n",
    "Now impute the values of the missing data in the remaining numeric columns using k-nearest neighbors with `KNNImputer()`. Use the 5 nearest neighbors. This allows us to implement the knn algorithm to predict the missing values for an observation based on similar complete observations.Perform and print a check to ensure that there are no more NA values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create target variable and numerical encoding \n",
    "Create the target variable (`nClimates`) representing the  number of climates in which a species was present. Then finish preparing the data to be used in our models.\n",
    "\n",
    "- Create a new column `nClimates` that combines the information of the five climate type variables\n",
    "- Drop the original `Dry`, `Humid`, `Montane`, `Subarid`, and `Subhumid` columns\n",
    "- Encode categorical variables\n",
    "- Split the dataset into training and test sets. Use a random state of 808. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train a decision tree\n",
    "Train a single decision tree to predict the number of climate types inhabited by each species.\n",
    "\n",
    "- Train a `DecisionTreeRegressor`.  Let's apply a constraint on its growth by not allowing the tree to grow further than 5 levels. Use a random state of 808. \n",
    "- Make predictions\n",
    "- Visualize the decision tree using `plot_tree`\n",
    "- Evaluate and print its performance using mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the way this tree looks, what would you guess the most important variable to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Random forest with hyperparameter tuning\n",
    "Now we'll use a more sophisticated ensemble approach where we optimize the values of key hyperparameters that govern tree growth.\n",
    "\n",
    "- Define a parameter grid that includes the follow parameters and values:\n",
    "  -  (sqrt(p), 6, and no set #) of the number of features to try at each split\n",
    "  -  (50, 100, 200) total trees (learners)\n",
    "  -  a maximum tree depth of (3,4,5,6,7)\n",
    "  -  (2,5,10) minimum samples per split\n",
    "  -  (1,2,4) minimum samples per leaf\n",
    "  \n",
    "- Use `GridSearchCV` with 5-fold cross-validation and mse (`neg_mean_squared_error`) to find the best combination of parameter values\n",
    "- Train the best estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Make predictions and evaluate random forest model\n",
    "Use the optimized random forest model (`best_rf`) to predict the number of climates a species inhabits and analyze its performance.\n",
    "\n",
    "- Use `best_rf` and `predict()` method on `X_test` to generate predictions\n",
    "- Print the best set of parameter values using thte `best_params` method\n",
    "- Compute and print the mse to evaluate model performance\n",
    "- Extract feature importance from the trained random forest model\n",
    "- Visualize feature importance using a bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model had better accuracy, the single tree or the ensemble.  Interpret which traits have the most influence on species' climate adaptability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
