{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 9: Predicting Forest Cover Type with SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In this lab, we will explore the application of Support Vector Machines (SVMs) and Random Forests (RFs) for multi-class classification using cartographic variables. Specifically, we will predict forest cover type based on a variety of environmental features such as elevation, soil type, and land aspect.\n",
    "\n",
    "Understanding forest cover classification is crucial for natural resource management. Land managers and conservationists rely on accurate predictions of vegetation types to make informed decisions about wildlife habitats, fire management, and sustainable forestry practices. However, direct field assessments of forest cover can be costly and time-consuming, making predictive models a valuable tool for estimating cover types in large or inaccessible regions.\n",
    "\n",
    "Dataset info here: https://archive.ics.uci.edu/dataset/31/covertype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"data/covtype_sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Preprocessing \n",
    "\n",
    "Before building our classification models, we need to prepare the dataset by separating the features target variable (`Cover_Type`) and  splitting the data into training and test sets. \n",
    "\n",
    "We didn't explicitly discuss it in lecture, but SVMs are sensitive to feature scale.  Use `describe()` to summarize the dataset.  Do you see anything that would require scaling of the data?  If so, apply that transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>Wilderness_Area_Rawah</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type_32</th>\n",
       "      <th>Soil_Type_33</th>\n",
       "      <th>Soil_Type_34</th>\n",
       "      <th>Soil_Type_35</th>\n",
       "      <th>Soil_Type_36</th>\n",
       "      <th>Soil_Type_37</th>\n",
       "      <th>Soil_Type_38</th>\n",
       "      <th>Soil_Type_39</th>\n",
       "      <th>Soil_Type_40</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2955.599500</td>\n",
       "      <td>154.450000</td>\n",
       "      <td>14.114700</td>\n",
       "      <td>268.097600</td>\n",
       "      <td>45.755300</td>\n",
       "      <td>2319.360300</td>\n",
       "      <td>212.19660</td>\n",
       "      <td>223.113500</td>\n",
       "      <td>142.243800</td>\n",
       "      <td>1960.040200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091200</td>\n",
       "      <td>0.082900</td>\n",
       "      <td>0.00250</td>\n",
       "      <td>0.00250</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.026800</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>2.036600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>281.786673</td>\n",
       "      <td>111.851861</td>\n",
       "      <td>7.499705</td>\n",
       "      <td>211.899673</td>\n",
       "      <td>58.034207</td>\n",
       "      <td>1548.558651</td>\n",
       "      <td>26.98846</td>\n",
       "      <td>19.871067</td>\n",
       "      <td>37.799752</td>\n",
       "      <td>1320.535941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287908</td>\n",
       "      <td>0.275745</td>\n",
       "      <td>0.04994</td>\n",
       "      <td>0.04994</td>\n",
       "      <td>0.014141</td>\n",
       "      <td>0.022356</td>\n",
       "      <td>0.161507</td>\n",
       "      <td>0.154603</td>\n",
       "      <td>0.115829</td>\n",
       "      <td>1.383782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1860.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-164.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68.00000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2804.750000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1091.750000</td>\n",
       "      <td>198.00000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>1006.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2995.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>1977.000000</td>\n",
       "      <td>218.00000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>1699.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3159.000000</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>3279.000000</td>\n",
       "      <td>231.00000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>2524.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3846.000000</td>\n",
       "      <td>359.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>1243.000000</td>\n",
       "      <td>427.000000</td>\n",
       "      <td>7078.000000</td>\n",
       "      <td>254.00000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>7111.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Elevation        Aspect         Slope  \\\n",
       "count  10000.000000  10000.000000  10000.000000   \n",
       "mean    2955.599500    154.450000     14.114700   \n",
       "std      281.786673    111.851861      7.499705   \n",
       "min     1860.000000      0.000000      0.000000   \n",
       "25%     2804.750000     58.000000      9.000000   \n",
       "50%     2995.000000    126.000000     13.000000   \n",
       "75%     3159.000000    258.000000     18.000000   \n",
       "max     3846.000000    359.000000     65.000000   \n",
       "\n",
       "       Horizontal_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "count                      10000.000000                     10000.000000   \n",
       "mean                         268.097600                        45.755300   \n",
       "std                          211.899673                        58.034207   \n",
       "min                            0.000000                      -164.000000   \n",
       "25%                           95.000000                         7.000000   \n",
       "50%                          218.000000                        29.000000   \n",
       "75%                          384.000000                        68.000000   \n",
       "max                         1243.000000                       427.000000   \n",
       "\n",
       "       Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "count   10000.000000     10000.00000   10000.000000   \n",
       "mean     2319.360300       212.19660     223.113500   \n",
       "std      1548.558651        26.98846      19.871067   \n",
       "min         0.000000        68.00000      71.000000   \n",
       "25%      1091.750000       198.00000     213.000000   \n",
       "50%      1977.000000       218.00000     226.000000   \n",
       "75%      3279.000000       231.00000     237.000000   \n",
       "max      7078.000000       254.00000     254.000000   \n",
       "\n",
       "       Horizontal_Distance_To_Fire_Points  Wilderness_Area_Rawah  ...  \\\n",
       "count                        10000.000000           10000.000000  ...   \n",
       "mean                           142.243800            1960.040200  ...   \n",
       "std                             37.799752            1320.535941  ...   \n",
       "min                              0.000000               0.000000  ...   \n",
       "25%                            119.000000            1006.000000  ...   \n",
       "50%                            142.000000            1699.000000  ...   \n",
       "75%                            168.000000            2524.000000  ...   \n",
       "max                            246.000000            7111.000000  ...   \n",
       "\n",
       "       Soil_Type_32  Soil_Type_33  Soil_Type_34  Soil_Type_35  Soil_Type_36  \\\n",
       "count  10000.000000  10000.000000   10000.00000   10000.00000  10000.000000   \n",
       "mean       0.091200      0.082900       0.00250       0.00250      0.000200   \n",
       "std        0.287908      0.275745       0.04994       0.04994      0.014141   \n",
       "min        0.000000      0.000000       0.00000       0.00000      0.000000   \n",
       "25%        0.000000      0.000000       0.00000       0.00000      0.000000   \n",
       "50%        0.000000      0.000000       0.00000       0.00000      0.000000   \n",
       "75%        0.000000      0.000000       0.00000       0.00000      0.000000   \n",
       "max        1.000000      1.000000       1.00000       1.00000      1.000000   \n",
       "\n",
       "       Soil_Type_37  Soil_Type_38  Soil_Type_39  Soil_Type_40    Cover_Type  \n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000  \n",
       "mean       0.000500      0.026800      0.024500      0.013600      2.036600  \n",
       "std        0.022356      0.161507      0.154603      0.115829      1.383782  \n",
       "min        0.000000      0.000000      0.000000      0.000000      1.000000  \n",
       "25%        0.000000      0.000000      0.000000      0.000000      1.000000  \n",
       "50%        0.000000      0.000000      0.000000      0.000000      2.000000  \n",
       "75%        0.000000      0.000000      0.000000      0.000000      2.000000  \n",
       "max        1.000000      1.000000      1.000000      1.000000      7.000000  \n",
       "\n",
       "[8 rows x 55 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View data using describe\n",
    "df.describe()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe` method gives us the summary statistics for each column in our dataframe. From using this method, we are able to get a glimpse of each variables distributions. From looking at the information for each variable, I would say that they need to be scaled as they have different measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = df.drop(columns=['Cover_Type'])\n",
    "y = df['Cover_Type']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=808)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Hyperparameter Tuning for SVM\n",
    "To optimize our SVM model, we need to search for the best hyperparameters that maximize classification accuracy. Since SVM performance depends heavily on `C`, `kernel`, and `gamma`, we will use `GridSearchCV()` to systematically test different combinations. Initialize a cross validation object with 5 folds using `StratifiedKFold`. Check out how `StratifiedKFold` differs from `Kfold` [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html). \n",
    "\n",
    "Then, set up a grid to test different values of: \n",
    "- `C` (regularization strength): how strictly the model fits the training data\n",
    "  - Candidate parameter values: `(0.1, 1, 10, 100)`\n",
    "- `kernel` (decision boundary shape): compares linear and radial basis function shapes\n",
    "  - Candidate parameter values: (linear, rbf)\n",
    "- `gamma` (influence of training observations): influence of individual points on decision boundary\n",
    "  - Candidate parameter values: (scale, auto)\n",
    "\n",
    "As models and datasets become more complex, consideration of computation time becomes more important.  You'll use `time.time()` to measure the time required to fit the grid object.  \n",
    "\n",
    "**Print the best parameters from your model, as well as the time required to fit the grid object.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize CV object with 5 folds\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Set up the RBF kernel and hyperparameter tuning and fit\n",
    "param_grid_svm = {\n",
    "    'C': [0.1 , 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_search_svm = GridSearchCV(SVC(random_state=808),\n",
    "                               param_grid_svm, cv = cv, scoring='accuracy')\n",
    "\n",
    "# Start Time for model\n",
    "time_start = time.time()\n",
    "\n",
    "# Fit the model\n",
    "grid_search_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# End time for model \n",
    "time_end = time.time()\n",
    "\n",
    "# Best SVM model\n",
    "best_svm = grid_search_svm.best_estimator_\n",
    "print(f\"Best SVM model: {best_svm}\")\n",
    "\n",
    "# Print time taken\n",
    "print(f\"Total time taken: {time_end - time_start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build a fit a Random Forest for comparison\n",
    "\n",
    "Let's compare our SVM to a Random Forest classifier.  Create a grid for cross-validation with three hyperparameters of your choice to tune, along with three sensible values for each one.  \n",
    "**Print the best parameters from your model, as well as the time required to fit the grid object.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Model Predictions and Evaluation\n",
    "Now that you have trained and optimized both a SVM and RF model, you will evaluate their performances on the test set to prepare for model comparison. In this step, you will:\n",
    "- Use the best models from `GridSearchCV()` to make predictions on the test set\n",
    "- Generate a confusion matrix for each model to visualize classification performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Gather and display additional performance metrics\n",
    "Now display the accuracy score and training time required for each model to so we can compare the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Compare the models\n",
    "Now that we have trained, optimized, and evaluated both SVM and RF models, we will compare them based on overall accuracy, training time, and types of errors made.\n",
    "\n",
    "Based on these comparisons, which model is more suitable for this task?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
