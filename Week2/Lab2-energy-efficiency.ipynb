{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c0708e-dea9-4b21-966c-299bb273e08e",
   "metadata": {},
   "source": [
    "# EDS232 Lab 2: Regularized Regression and Energy Modeling\n",
    "\n",
    "\n",
    "## Overview\n",
    "In this lab, you will explore Ridge Regression and Lasso Regression, two common techniques for regularized regression. Both methods add penalties to the standard regression coefficients, which can help prevent overfitting and improve model interpretability.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "**Types of Regularization:**\n",
    "\n",
    "- Ridge Regression:\n",
    "\n",
    "    - Adds an L2 penalty ($\\lambda \\sum_{j=1}^p \\beta_j^2 $) to the regression loss function.\n",
    "    - Shrinks coefficients toward zero but does not set any coefficients exactly to zero.\n",
    "    - Best suited for reducing multicollinearity and improving model stability.\n",
    "\n",
    "- Lasso Regression:\n",
    "\n",
    "    - Adds an L1 penalty ($\\lambda \\sum_{j=1}^p |\\beta_j|$) to the regression loss function.\n",
    "    - Can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
    "    - Useful when you want a simpler, interpretable model.\n",
    "\n",
    "**Cross-Validation**\n",
    "\n",
    "- Cross-validation is a method for model evaluation and hyperparameter tuning.  In this lab we will be tuning $\\lambda$, which controls the penalty terms.\n",
    "\n",
    "- Note: Sci-kit learn refers to $\\lambda$, the regularization penalty hyperparameter, as \"alpha\" instead of \"lambda\" \n",
    "\n",
    "\n",
    "## About the data\n",
    "\n",
    "Buildings account for a significant portion of global energy consumption. Effective energy modeling and analysis are critical for designing energy-efficient buildings, reducing greenhouse gas emissions, and meeting sustainability goals. \n",
    "\n",
    "- This dataset contains energy performance data for 768 different building configurations, generated through simulation using Ecotect software. These configurations vary across multiple architectural features:\n",
    "\n",
    "| Feature                     | Description                                                                                   |\n",
    "|-----------------------------|-----------------------------------------------------------------------------------------------|\n",
    "| Relative Compactness        | A measure of how compact the building's shape is.                                             |\n",
    "| Surface Area                | The total external surface area of the building.                                              |\n",
    "| Wall Area                   | The total area of the walls.                                                                  |\n",
    "| Roof Area                   | The area of the roof.                                                                         |\n",
    "| Overall Height              | The height of the building.                                                                   |\n",
    "| Orientation                 | The direction the building faces (categorical, encoded numerically).                         |\n",
    "| Glazing Area                | The percentage of the buildingâ€™s facade that is glass.                                        |\n",
    "| Glazing Area Distribution   | The distribution of the glazing area (e.g., evenly distributed or concentrated on one side). |\n",
    "| Heating Load (Y1)           | Continuous target variable representing heating energy requirements. (This is the one we'll use for today's lab.) |\n",
    "| Cooling Load (Y2)           | Continuous target variable representing cooling energy requirements. (We won't be using this one.) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd7e68a-91ab-4566-a3a7-3e599191e6a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from ucimlrepo import fetch_ucirepo \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052cec6a",
   "metadata": {},
   "source": [
    "### Step 1: Fetch  dataset \n",
    "Grab the data from the UC Irvine Machine Learning Repository using fetch_ucrepo.  Note that the object returned contains both tabular data as well as metadata.  We'll need to add column names to the tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ecf0b-bce9-47ad-b902-d9dcf2448dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch dataset\n",
    "energy_efficiency = \n",
    "\n",
    "#Specify column names as a list of names from the metadata\n",
    "feature_names = [\"Relative Compactness\",\n",
    "    \"Surface Area\",\n",
    "    \"Wall Area\",\n",
    "    \"Roof Area\",\n",
    "    \"Overall Height\",\n",
    "    \"Orientation\",\n",
    "    \"Glazing Area\",\n",
    "    \"Glazing Area Distribution\",]\n",
    "\n",
    "#Add feature_names as column names in energy_efficiency.data\n",
    "energy_efficiency.data['features'].columns\n",
    "\n",
    "# Rename the 'targets' columns and assign Heating Load target to heating_load\n",
    "\n",
    "\n",
    "heating_load = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8127d4b-1a07-49d2-9eb9-9ceb58e37868",
   "metadata": {},
   "source": [
    "### Step 2: Preprocess the data\n",
    "Split the data into training and test sets **using a 70/30 split and a random state value of 42**. Then scale the data using StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2949568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Assign predictor and outcome variables \n",
    "X = \n",
    "Y = \n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = \n",
    "\n",
    "# Standardize the predictors\n",
    "scaler = \n",
    "X_train_scaled = \n",
    "X_test_scaled = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410fb7ba",
   "metadata": {},
   "source": [
    "### Step 3: Initial model fits\n",
    "We'll start by fitting a simple ordinary least squared regression model for comparison purposes.  Then we'll fit an initial ridge regression model. Let's start by choosing 10 as an arbitrary value for alpha. Recall that we can access coefficients from a model instance with `model_name.coef_`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decbe825",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create OLS instance and fit it\n",
    "ols = \n",
    "\n",
    "# Define a fixed alpha (lambda)\n",
    "alpha_fixed = \n",
    "\n",
    "# Create Ridge regression instance and fit it\n",
    "ridge = \n",
    "\n",
    "# Check coefficients of the OLS and ridge models\n",
    "print(\"OLS Coefficients:\", )\n",
    "print(\"Ridge Coefficients:\", )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ec75ba",
   "metadata": {},
   "source": [
    "Is the penalty term doing its job?  How can you tell?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22513ce9",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f77de4b",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate ridge model\n",
    "Now let's use our ridge model to make predictions and evaluate it using MSE. **Be sure to print your results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e19926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictions using ridge model\n",
    "y_train_pred = \n",
    "y_test_pred = \n",
    "\n",
    "# Evaluate MSE\n",
    "mse_train = \n",
    "mse_test = \n",
    "\n",
    "print(f\"Train MSE: \")\n",
    "print(f\"Test MSE: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c3469",
   "metadata": {},
   "source": [
    "### Step 5: Cross-validation with ridge\n",
    "Now let's move to a more sophisticated version of a ridge regression model using cross-validation using the RidgeCV class.  Let's try models with 0.1, 1.0, and 10.0 as values of alpha. Of the alpha parameter values we try, the model will save the that yielded the best MSE in the `_cv.alpha_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab3def6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the three alpha values\n",
    "alphas = \n",
    "\n",
    "# Fit RidgeCV\n",
    "ridge_cv = \n",
    "\n",
    "# Print best alpha\n",
    "\n",
    "\n",
    "# Evaluate model with the best alpha\n",
    "y_test_pred_cv = \n",
    "mse_test_cv = \n",
    "print(f\"Test MSE with best alpha:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a545998-596f-4021-83a9-71010807a5dc",
   "metadata": {},
   "source": [
    "### Step 6: Fit ridge models to a large range of alpha values \n",
    "Now we are going to dig a little deeper to see alpha's effect on the model coefficients. Let's define a larger range of alphas to try on a log scale with np.logspace(). Let's use 100 values ranging between -4 and 4.  Then using a for loop, we'll fit a Ridge model for each of those values of alpha, appending each model's coefficients (`coefficients.append()`) as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de186e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define alphas for ridge regression.  Initialize an empty list to hold our coefficients\n",
    "alphas =   # Alphas from 0.0001 to 10,000\n",
    "coefficients = \n",
    "\n",
    "# Fit a ridge model for each alpha and collect coefficients\n",
    "for alpha in alphas:\n",
    "\n",
    "\n",
    "coefficients = \n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4209c1",
   "metadata": {},
   "source": [
    "### Step 7: Plot ridge coefficients vs. lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f286aa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot coefficients vs. alpha. Another for loop may be in order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d6b08",
   "metadata": {},
   "source": [
    "### Step 8: Run a full cross-validated ridge model using RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd16f9a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit a RidgeCV with cross-validation\n",
    "ridge_cv = \n",
    "\n",
    "\n",
    "# Print the optimal alpha and coefficients corresponding to that alpha\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set by predicting..\n",
    "y_test_pred = \n",
    "\n",
    "# ... and printing the MSE\n",
    "mse = \n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2d8b18-e8ed-4b95-8cd3-afc3dc83f456",
   "metadata": {},
   "source": [
    "### Step 9: Fit lasso regression using LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a5a2e-1995-4f8c-9d5d-10f72c1d2f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit lasso regression with cross-validation\n",
    "lasso_cv = \n",
    "\n",
    "# Print the optimal alpha and associated coefficients\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4691e6-47ae-44ca-8446-9d950c1ea441",
   "metadata": {},
   "source": [
    "### Step 10: The \"one-standard error\" rule \n",
    "We can use 1-SE rule to trade off a small amount of accuracy for increased model simplicity and interpretability.  To do so, find the highest penalty value that falls within 1 standard error of the minimum MSE.  Hopefully it will reduce the number of features in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c8a3f-40f1-41bd-b535-6a523f93672a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Think: what is this code doing?\n",
    "lasso_best_alpha_1se = lasso_cv.alphas_[np.where(\n",
    "    lasso_cv.mse_path_.mean(axis=1) <= (lasso_cv.mse_path_.mean(axis=1).min() + lasso_cv.mse_path_.std(axis=1).mean())\n",
    ")[0][0]]\n",
    "\n",
    "# Fit a Lasso with this new alpha\n",
    "lasso_1se_model = \n",
    "\n",
    "# Identify remaining features for 1-SE rule (those with non-zero coefficients)\n",
    "remaining_features_1se = \n",
    "print(remaining_features_1se)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9975e6f2-ae06-421c-a872-5fc08fc38573",
   "metadata": {},
   "source": [
    "### Step 11: Compare the performance of your ridge and lasso regression models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65f94f",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
