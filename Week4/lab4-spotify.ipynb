{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7fbe2b-da60-4059-ae71-e322fecfd0db",
   "metadata": {},
   "source": [
    "# Lab 4: Building a Spotify Song Classifier with KNNs and Decision Trees\n",
    "**Classify by genres**. In this lab, you will build machine learning genre classifiers using a dataset of Spotify tracks. Your goal is to train a model that can distinguish between two selected genres based on various audio features such as danceability, energy, valence, tempo, and more.\n",
    "\n",
    "We'll follow the familiar workflow of data exploration -> preprocessing -> model training -> evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb34875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaac8c4-fbb4-40f9-8a59-e800395c6c65",
   "metadata": {},
   "source": [
    "### Step 1: The data\n",
    "We'll focus on two genres: Blues and Jazz (what’s the difference, anyway??).\n",
    "\n",
    "Both genres have rich musical histories, but they differ in structure:\n",
    "\n",
    "- Blues tends to have simpler chord progressions and a strong emotional expression.\n",
    "- Jazz often involves complex improvisation and more varied rhythms.\n",
    "\n",
    "But can a machine really tell them apart just by looking at numerical audio features? That’s what we’ll find out!\n",
    "\n",
    "In this step, you will:\n",
    "- Load the dataset\n",
    "- Extract only blues and jazz for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082e215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data = pd.read_csv('/courses/EDS232/Data/spotify_dat.csv')\n",
    "\n",
    "# Print the data types in spotify_data\n",
    "\n",
    "\n",
    "# Filter down to two genres\n",
    "blues_data = \n",
    "jazz_data =  \n",
    "\n",
    "# Combine both genres into one dataset\n",
    "combined_data =  \n",
    "\n",
    "# Rename 'genres' column to 'genre' for clarity\n",
    "combined_data = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e8340d-f51a-4659-b3eb-c0549575552a",
   "metadata": {},
   "source": [
    "## Step 2: Data Exploration\n",
    "Now that we have Blues and Jazz as our two genres, it's time to explore the data and look for patterns in the audio features.\n",
    "\n",
    "Let's investigate:\n",
    "- Which features are available?\n",
    "- How strongly these features correlate with each other.\n",
    "- How different genres distribute across  sample features: danceability \n",
    "\n",
    "In this step, we will:\n",
    "- Filter out non-numeric columns before the plotting\n",
    "- Visualize feature correlations using a heatmap.\n",
    "- Examine key feature distributions with histograms to see if genres behave differently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d476df1-a888-43a2-aabb-987bc9e835aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude non-numeric columns but keep 'genre' for grouping\n",
    "numeric_features = \n",
    "numeric_features['genre'] =\n",
    "\n",
    "#Correlation heatmap\n",
    "\n",
    "\n",
    "#Histogram of danceability\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f79f22-1036-4ae9-90f8-73ad0f6169b0",
   "metadata": {},
   "source": [
    "### Step 3: Check for class imbalance\n",
    "An unequal distribution between the target classes can cause issues for our classifiers, so we'll need to check for that. A commonly used guideline for class imbalance is:\n",
    "\n",
    "- Slight Imbalance (Less than 1.5:1) → Generally okay; no need for balancing.\n",
    "- Moderate Imbalance (Between 1.5:1 and 3:1) → Might cause some bias; balancing can help.\n",
    "- Severe Imbalance (More than 3:1) → Strongly affects model performance; balancing is usually necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1470168c-bd6c-412a-9549-47ff191e8644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print counts of each genre\n",
    "class_counts = \n",
    "\n",
    "# Calculate ratio of majority to minority class\n",
    "class_ratio = \n",
    "\n",
    "# Print class distribution and ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20e289d",
   "metadata": {},
   "source": [
    "Given this result, should we be worried about the imbalance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3583ec",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d0f97",
   "metadata": {},
   "source": [
    "### Step 4: Preprocessing\n",
    "We need to ensure all the variables we use are numerical so the models can process them. First we'll drop all the variables that are not numeric or the target. Then we'll use a sklearn utility, `LabelEncoder()` to encode our categorical target variable into numerical values. Print the first five values of your encoded labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce674f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the features (drop columns that are not numeric or target)\n",
    "X = \n",
    "\n",
    "# Set the target variable as 'genre'\n",
    "y =  \n",
    "\n",
    "# Encode the target variable ('label') using label encoding\n",
    "label_encoder = \n",
    "y_encoded =\n",
    "\n",
    "# Print first five values of encoded labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee85305",
   "metadata": {},
   "source": [
    "Good, now we'll do our data splitting and scaling.  Let's go with an 80/20 split this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0cf1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into 80% train and 20% test\n",
    "X_train, X_test, y_train, y_test = \n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = \n",
    "\n",
    "# Fit and transform only the training set\n",
    "X_train_scaled = \n",
    "\n",
    "# Use the same scaler to transform the test set\n",
    "X_test_scaled = \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c0a3c-c1b9-438a-8e44-5654e6406f6d",
   "metadata": {},
   "source": [
    "### Step 5: Train the models\n",
    "OK, now it's time to build our models and assess the training accuracy. We'll use the default of k = 5 for the KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f88e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "knn = \n",
    "dt = \n",
    "\n",
    "# Train (fit) both models\n",
    "\n",
    "\n",
    "# Predictions on training data\n",
    "knn_y_train_pred = \n",
    "dt_y_train_pred = \n",
    "\n",
    "# Compute training accuracy\n",
    "knn_train_accuracy = \n",
    "dt_train_accuracy = \n",
    "\n",
    "#Print training accuracy for both models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ec565",
   "metadata": {},
   "source": [
    "### Step 6: Evaluate models\n",
    "OK, now let's go ahead and predict on the test data to see how well our models do at predicting the genre of unseen tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce64ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "knn_pred = \n",
    "dt_pred = \n",
    "\n",
    "# Evaluate prediction accuracy\n",
    "knn_accuracy = \n",
    "dt_accuracy = \n",
    "\n",
    "#Print prediction accuracy for both models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aed7de",
   "metadata": {},
   "source": [
    "### Step 7: Visualizing Model Performance\n",
    "Now that we have evaluated our models on unseen data, let’s compare their performance by visualizing the training vs. test accuracy.  We'll look for signs of overfitting by plotting the training accuracy and test accuracy for both k-NN and Decision Trees.\n",
    "\n",
    "Create a bar chart that compares the training accuracy and test accuracy for k-NN and Decision Trees. Your plot should clearly display:\n",
    "\n",
    "- The training and test accuracy for both models.\n",
    "- Properly labeled axes and a title.\n",
    "- A legend to differentiate training and test accuracy.\n",
    "- Accuracy values displayed on top of each bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4efa80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bea31ee",
   "metadata": {},
   "source": [
    "Which model does a better job of generalizing to unseen data? How do you know?\n",
    "Does it appear that either of the models are overfitting? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6ab978",
   "metadata": {},
   "source": [
    "*your anwer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fb937d-c97c-49f8-9376-1b03b51b0226",
   "metadata": {},
   "source": [
    "### Bonus question\n",
    "In the workflow above, we used the default parameter values of k = 5 for the KNN model, and maxdepth = None for the Decision Tree. Choose one of the models and see if you can improve test performance by optmizing its corresponding parameter value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
